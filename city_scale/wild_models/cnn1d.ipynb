{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = \"/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_8jun_300_win_300twin.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_435494/1329464943.py:1: DtypeWarning: Columns (129,131,132,134) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(processed_dataset)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[315:]\n",
    "Y = df2[\"accident_label\"]\n",
    "X = df2.drop([\"incident_edge\",\"incident_start_time\",\"incident_type\",\"accident_id\",\"accident_duration\",\"incident_lane\",\"accident_label\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop([\"rolling_travel_time_32_16\"],axis=1)\n",
    "X = X.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing df\n",
    "\n",
    "def standardize(col):\n",
    "   return (col - col.mean()) / col.std()\n",
    "\n",
    "X = X.apply(standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                        0\n",
      "step                              0\n",
      "rolling_junction_mean_speed_0     0\n",
      "rolling_traffic_count_0           0\n",
      "rolling_traffic_occupancy_0       0\n",
      "rolling_junction_mean_speed_1     0\n",
      "rolling_traffic_count_1           0\n",
      "rolling_traffic_occupancy_1       0\n",
      "rolling_junction_mean_speed_2     0\n",
      "rolling_traffic_count_2           0\n",
      "rolling_traffic_occupancy_2       0\n",
      "rolling_junction_mean_speed_3     0\n",
      "rolling_traffic_count_3           0\n",
      "rolling_traffic_occupancy_3       0\n",
      "rolling_junction_mean_speed_4     0\n",
      "rolling_traffic_count_4           0\n",
      "rolling_traffic_occupancy_4       0\n",
      "rolling_junction_mean_speed_5     0\n",
      "rolling_traffic_count_5           0\n",
      "rolling_traffic_occupancy_5       0\n",
      "rolling_junction_mean_speed_6     0\n",
      "rolling_traffic_count_6           0\n",
      "rolling_traffic_occupancy_6       0\n",
      "rolling_junction_mean_speed_7     0\n",
      "rolling_traffic_count_7           0\n",
      "rolling_traffic_occupancy_7       0\n",
      "rolling_junction_mean_speed_8     0\n",
      "rolling_traffic_count_8           0\n",
      "rolling_traffic_occupancy_8       0\n",
      "rolling_junction_mean_speed_9     0\n",
      "rolling_traffic_count_9           0\n",
      "rolling_traffic_occupancy_9       0\n",
      "rolling_junction_mean_speed_10    0\n",
      "rolling_traffic_count_10          0\n",
      "rolling_traffic_occupancy_10      0\n",
      "rolling_junction_mean_speed_11    0\n",
      "rolling_traffic_count_11          0\n",
      "rolling_traffic_occupancy_11      0\n",
      "rolling_junction_mean_speed_12    0\n",
      "rolling_traffic_count_12          0\n",
      "rolling_traffic_occupancy_12      0\n",
      "rolling_junction_mean_speed_13    0\n",
      "rolling_traffic_count_13          0\n",
      "rolling_traffic_occupancy_13      0\n",
      "rolling_junction_mean_speed_14    0\n",
      "rolling_traffic_count_14          0\n",
      "rolling_traffic_occupancy_14      0\n",
      "rolling_junction_mean_speed_15    0\n",
      "rolling_traffic_count_15          0\n",
      "rolling_traffic_occupancy_15      0\n",
      "rolling_junction_mean_speed_16    0\n",
      "rolling_traffic_count_16          0\n",
      "rolling_traffic_occupancy_16      0\n",
      "rolling_junction_mean_speed_17    0\n",
      "rolling_traffic_count_17          0\n",
      "rolling_traffic_occupancy_17      0\n",
      "rolling_junction_mean_speed_18    0\n",
      "rolling_traffic_count_18          0\n",
      "rolling_traffic_occupancy_18      0\n",
      "rolling_junction_mean_speed_19    0\n",
      "rolling_traffic_count_19          0\n",
      "rolling_traffic_occupancy_19      0\n",
      "rolling_junction_mean_speed_20    0\n",
      "rolling_traffic_count_20          0\n",
      "rolling_traffic_occupancy_20      0\n",
      "rolling_junction_mean_speed_21    0\n",
      "rolling_traffic_count_21          0\n",
      "rolling_traffic_occupancy_21      0\n",
      "rolling_junction_mean_speed_22    0\n",
      "rolling_traffic_count_22          0\n",
      "rolling_traffic_occupancy_22      0\n",
      "rolling_junction_mean_speed_23    0\n",
      "rolling_traffic_count_23          0\n",
      "rolling_traffic_occupancy_23      0\n",
      "rolling_junction_mean_speed_24    0\n",
      "rolling_traffic_count_24          0\n",
      "rolling_traffic_occupancy_24      0\n",
      "rolling_junction_mean_speed_25    0\n",
      "rolling_traffic_count_25          0\n",
      "rolling_traffic_occupancy_25      0\n",
      "rolling_junction_mean_speed_26    0\n",
      "rolling_traffic_count_26          0\n",
      "rolling_traffic_occupancy_26      0\n",
      "rolling_junction_mean_speed_27    0\n",
      "rolling_traffic_count_27          0\n",
      "rolling_traffic_occupancy_27      0\n",
      "rolling_junction_mean_speed_28    0\n",
      "rolling_traffic_count_28          0\n",
      "rolling_traffic_occupancy_28      0\n",
      "rolling_junction_mean_speed_29    0\n",
      "rolling_traffic_count_29          0\n",
      "rolling_traffic_occupancy_29      0\n",
      "rolling_junction_mean_speed_30    0\n",
      "rolling_traffic_count_30          0\n",
      "rolling_traffic_occupancy_30      0\n",
      "rolling_junction_mean_speed_31    0\n",
      "rolling_traffic_count_31          0\n",
      "rolling_traffic_occupancy_31      0\n",
      "rolling_junction_mean_speed_32    0\n",
      "rolling_traffic_count_32          0\n",
      "rolling_traffic_occupancy_32      0\n",
      "rolling_junction_mean_speed_33    0\n",
      "rolling_traffic_count_33          0\n",
      "rolling_traffic_occupancy_33      0\n",
      "rolling_junction_mean_speed_34    0\n",
      "rolling_traffic_count_34          0\n",
      "rolling_traffic_occupancy_34      0\n",
      "rolling_junction_mean_speed_35    0\n",
      "rolling_traffic_count_35          0\n",
      "rolling_traffic_occupancy_35      0\n",
      "rolling_junction_mean_speed_36    0\n",
      "rolling_traffic_count_36          0\n",
      "rolling_traffic_occupancy_36      0\n",
      "rolling_junction_mean_speed_37    0\n",
      "rolling_traffic_count_37          0\n",
      "rolling_traffic_occupancy_37      0\n",
      "rolling_junction_mean_speed_38    0\n",
      "rolling_traffic_count_38          0\n",
      "rolling_traffic_occupancy_38      0\n",
      "rolling_junction_mean_speed_39    0\n",
      "rolling_traffic_count_39          0\n",
      "rolling_traffic_occupancy_39      0\n",
      "rolling_junction_mean_speed_40    0\n",
      "rolling_traffic_count_40          0\n",
      "rolling_traffic_occupancy_40      0\n",
      "rolling_junction_mean_speed_41    0\n",
      "rolling_traffic_count_41          0\n",
      "rolling_traffic_occupancy_41      0\n",
      "time_of_day                       0\n",
      "rolling_travel_time_3_5           0\n",
      "rolling_travel_time_10_4          0\n",
      "rolling_travel_time_8_11          0\n",
      "rolling_travel_time_14_9          0\n",
      "rolling_travel_time_30_33         0\n",
      "rolling_travel_time_38_31         0\n",
      "rolling_travel_time_34_39         0\n",
      "rolling_travel_time_42_35         0\n",
      "rolling_travel_time_32_2          0\n",
      "rolling_travel_time_1_29          0\n",
      "rolling_travel_time_26_13         0\n",
      "rolling_travel_time_12_27         0\n",
      "rolling_travel_time_40_25         0\n",
      "rolling_travel_time_24_41         0\n",
      "rolling_travel_time_6_22          0\n",
      "rolling_travel_time_21_7          0\n",
      "rolling_travel_time_1_16          0\n",
      "rolling_travel_time_15_2          0\n",
      "rolling_travel_time_19_23         0\n",
      "rolling_travel_time_28_20         0\n",
      "rolling_travel_time_15_29         0\n",
      "rolling_travel_time_36_18         0\n",
      "rolling_travel_time_17_37         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(Y.values[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        # Convert the dataframes to tensors once, if they fit in memory\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y.values, dtype=torch.long)  # Use torch.long for classification indices\n",
    "        \n",
    "        self.num_files = len(self.X)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Directly access the pre-converted tensors\n",
    "        row = self.X[index]\n",
    "        label_data = self.Y[index]\n",
    "        \n",
    "        return row, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA available : {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_tabular(nn.Module):\n",
    "    def __init__(self,input_features,output_features,num_classes):\n",
    "        super(CNN_tabular, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_features, output_features)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) \n",
    "        # Optional: Define a pooling layer\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Calculate the size of the flattened features after convolutions and pooling\n",
    "        self.flattened_size = 32 * (expanded_features // 2)  # Assuming one pooling layer\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        expanded = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = x.unsqueeze(1)  # Add a channel dimension\n",
    "        \n",
    "        # Convolution layers with activation function\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)  # Applying pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Flatten the output for dense layers\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x),dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test splits\n",
    "\n",
    "dataset = TrafficDataset(X,Y)\n",
    "\n",
    "# Calculate the lengths of splits\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.9 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "print(f\"Train size is : {train_size}\")\n",
    "print(f\"Test size is : {test_size}\")\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for both datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0) # num of workers = 0 in windows\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0) # Shuffle is usually False for testing/validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs  = 40\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "model = CNN_tabular()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=weights)\n",
    "# loss_function = FocalLoss(alpha=weights, gamma=3)\n",
    "lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.to(device)\n",
    "correct_predictions_per_class = torch.zeros(n_classes, device=device)\n",
    "actual_per_class = torch.zeros(n_classes, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Device used is {device}\")\n",
    "global_step = 0\n",
    "training_loss_data = [] # Stores total training loss for each epoch\n",
    "testing_loss_data = [] # Stores total testing loss for each epoch\n",
    "epochs = []\n",
    "training_time = 0 # Stores the total training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training started for {num_epochs} epochs\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epochs .append(epoch) \n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "    total_predictions =0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Move data to the device\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.data\n",
    "        _, labels_indices = labels\n",
    "\n",
    "        if predicted >= 0.5:\n",
    "            predicted =1\n",
    "            \n",
    "        \n",
    "        \n",
    "        predicted = predicted.to(labels_indices.device).long() \n",
    "        labels_indices = labels_indices.to(predicted.device).long()\n",
    "\n",
    "        correct_predictions += (predicted == labels_indices).sum().item()\n",
    "        total_predictions = total_predictions+32\n",
    "\n",
    "\n",
    "        if i % 100 == 0:  # Log every 10 batches\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1}/{len(train_dataloader)}, Partial Loss: {running_loss/(i+1):.4f}, Correct Predictions: {correct_predictions}/{total_predictions}\")\n",
    "            \n",
    "\n",
    "        global_step+=1\n",
    "\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    training_loss_data.append(epoch_loss)\n",
    "    epoch_accuracy = (correct_predictions / total_samples) * 100\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds.\")\n",
    "    # print(f\"Before Learning rate {before_lr}, After Learning rate {after_lr}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    # Testing phase ---------------------------------------------------------------------------\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        correct_test_predictions = 0\n",
    "        total_test_samples = 0\n",
    "        test_loss = 0\n",
    "        for test_inputs, test_labels in test_dataloader:\n",
    "            test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)  \n",
    "            outputs = model(test_inputs)\n",
    "            loss = loss_function(outputs, test_labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.data\n",
    "            if predicted >= 0.5:\n",
    "                predicted = 1\n",
    "            total_test_samples += test_labels.size(0)\n",
    "            _, labels_indices = test_labels\n",
    "\n",
    "            correct_test_predictions += (predicted == labels_indices).sum().item()\n",
    "    testing_loss_data.append(test_loss/len(test_dataloader))\n",
    "    test_accuracy = (correct_test_predictions / total_test_samples) * 100\n",
    "    print(f\"Epoch {epoch+1} Test Accuracy: {test_accuracy:.2f}% Test Loss {test_loss/len(test_dataloader)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss vs epochs\n",
    "plt.plot(epochs,training_loss_data)\n",
    "plt.plot(epochs,testing_loss_data)\n",
    "plt.title(\"Training loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_3jun_300_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_3jun_600_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_3jun_600_win_600twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_3jun_900_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_4jun_300_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_4jun_600_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_4jun_600_win_600twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_4jun_900_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_5jun_300_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_5jun_600_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_5jun_600_win_600twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_5jun_900_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_6jun_300_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_6jun_600_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_6jun_600_win_600twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_6jun_900_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_7jun_300_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_7jun_600_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_7jun_600_win_600twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_7jun_900_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_8jun_300_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_8jun_600_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_8jun_600_win_600twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/2024-2-16_1915hours_8jun_900_win_300twin.csv', '/home/local/ASURITE/speddira/dev/traffic_sense_net/city_scale/processed_datasets/incidents_2024-2-16_1915hours_2592000steps.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
