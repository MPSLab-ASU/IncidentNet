{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "writer = SummaryWriter('runs/exp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT PARAMETERS\n",
    "\n",
    "experiment_no = 10\n",
    "features_path = r\"data/dataset3/features\"\n",
    "labels_path = r\"data/dataset3/labels\"\n",
    "travel_time_path = r\"data/dataset3/travel_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed for the experiment\n"
     ]
    }
   ],
   "source": [
    "set_seed = True\n",
    "\n",
    "seed = 20\n",
    "if set_seed:\n",
    "    print(\"Setting seed for the experiment\")\n",
    "    # Set seed for CPU and all GPUs (if available)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Convert inputs and targets to probabilities\n",
    "        \n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        targets = torch.argmax(targets,axis=1)\n",
    "        \n",
    "        alpha = self.alpha[targets].to(inputs.device)\n",
    "\n",
    "        \n",
    "        # Calculate pt\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        # Calculate Focal Loss\n",
    "        focal_loss = alpha * (1 - pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:  # 'none'\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, directory,travel_time_directory,label_directory):\n",
    "        \n",
    "        self.files = [os.path.join(directory, f) for f in sorted(os.listdir(directory))] #[0:100]\n",
    "        self.travel_time_files = [os.path.join(travel_time_directory, f) for f in sorted(os.listdir(travel_time_directory))]\n",
    "        self.yfiles= [os.path.join(label_directory, f) for f in sorted(os.listdir(label_directory))] #[0:100]\n",
    "        self.num_files = len(self.files)\n",
    "        \n",
    "        self.total_sequences = self.num_files # * self.sequences_per_file\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_sequences\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        file_index = index \n",
    "        \n",
    "        data = np.load(self.files[file_index]) #Loads the file which contains our desired sequence\n",
    "        label_data = np.load(self.yfiles[file_index])\n",
    "        travel_time_data = np.load(self.travel_time_files[file_index])\n",
    "\n",
    "        sequence = np.transpose(data, (2, 0, 1)) # To orient in the dimension with 3 channels being first\n",
    "        \n",
    "        #When majority class is important\n",
    "        # label_data_summed = np.sum(label_data,axis=0)\n",
    "        # indx = np.argmax(label_data_summed)\n",
    "        # labelsWindow = np.zeros(13)\n",
    "        # labelsWindow[indx] = 1\n",
    "        \n",
    "        \n",
    "        #When latest trend is important\n",
    "        # last_row = label_data[-1:]\n",
    "        # indx = np.argmax(last_row)\n",
    "        # labelsWindow = np.zeros(13)\n",
    "        # labelsWindow[indx] = 1\n",
    "        \n",
    "        # Just checking if it can predict accident or not based on latest row class\n",
    "        last_row = label_data[-1:]\n",
    "        indx = np.argmax(last_row)\n",
    "        labelsWindow = np.zeros(2)\n",
    "        if indx != 12:\n",
    "            labelsWindow[0] = 1 # Accident true \n",
    "        else:\n",
    "            labelsWindow[1] = 1 # Accident false\n",
    "        \n",
    "        \n",
    "        # Travel Time lets take the last row row\n",
    "        \n",
    "        travel_time_data = travel_time_data[-1]\n",
    "        \n",
    "        return torch.tensor(sequence, dtype=torch.float32),torch.tensor(labelsWindow, dtype=torch.float32),torch.tensor(travel_time_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA available : {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # Assuming the input shape is (batch_size, 3, 300, 42) after reshaping\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=96, kernel_size=(3, 3), stride=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=96, out_channels=128, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.fc1 = nn.Linear(1,1024)\n",
    "        self.fc3 = nn.Linear(1024, 600)\n",
    "        self.lstm = nn.LSTM(input_size=600, hidden_size=128, num_layers=1, batch_first=True)  \n",
    "        self.fc5 = nn.Linear(128,64)\n",
    "        self.fc6 = nn.Linear(64, 13)\n",
    "        self.fc7 = nn.Linear(13, 2) # Output layer\n",
    "\n",
    "    def forward(self, x,travel_times):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # Flatten starting from the 2nd dimension\n",
    "\n",
    "        # Dynamically determine 'in_features' for the first fully connected layer\n",
    "        if self.fc1.in_features == 1:  # Check if the placeholder value is still there\n",
    "            num_features = x.shape[1]\n",
    "            self.fc1 = nn.Linear(num_features, 1024).to(x.device)  # Replace with the correct in_features\n",
    "        \n",
    "        x = F.gelu(self.fc1(x))\n",
    "        # x = F.gelu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.fc3(x))\n",
    "    \n",
    "        x,_ = self.lstm(x)\n",
    "        # x = torch.cat((x,travel_times),dim=1)\n",
    "        # if self.fc4.in_features == 1:  # Check if the placeholder value is still there\n",
    "        #     num_features = x.shape[1]\n",
    "        #     self.fc4 = nn.Linear(num_features, 128).to(x.device)\n",
    "        \n",
    "        # x = self.dropout(x)\n",
    "        # x = F.gelu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.fc5(x))\n",
    "        x = F.gelu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features path used is data/dataset4/features\n",
      "Labels path used is data/dataset4/labels\n",
      "travel_time_path path used is data/dataset3/travel_time\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/dataset4/features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels path used is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtravel_time_path path used is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtravel_time_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTrafficDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtravel_time_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate the lengths of splits\u001b[39;00m\n\u001b[1;32m      8\u001b[0m total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mTrafficDataset.__init__\u001b[0;34m(self, directory, travel_time_directory, label_directory)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory,travel_time_directory,label_directory):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m)] \u001b[38;5;66;03m#[0:100]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtravel_time_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(travel_time_directory, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(travel_time_directory))]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myfiles\u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(label_directory, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(label_directory))] \u001b[38;5;66;03m#[0:100]\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dataset4/features'"
     ]
    }
   ],
   "source": [
    "# Train - Test splits\n",
    "print(f'Features path used is {features_path}')\n",
    "print(f'Labels path used is {labels_path}')\n",
    "print(f'travel_time_path path used is {travel_time_path}')\n",
    "dataset = TrafficDataset(features_path,travel_time_path, labels_path)\n",
    "\n",
    "# Calculate the lengths of splits\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.9 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "print(f\"Train size is : {train_size}\")\n",
    "print(f\"Test size is : {test_size}\")\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for both datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0) # num of workers = 0 in windows\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0) # Shuffle is usually False for testing/validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# classes_count_dict = {\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     0:0,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     1:0,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     12:0\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     16\u001b[0m classes_count_dict   \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m }\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(\u001b[38;5;28minput\u001b[39m,labels,travel_time) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m):\n\u001b[1;32m     23\u001b[0m     _, labels_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(labels, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m labels_indices:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# print(int(index.numpy()))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# classes_count_dict = {\n",
    "#     0:0,\n",
    "#     1:0,\n",
    "#     2:0,\n",
    "#     3:0,\n",
    "#     4:0,\n",
    "#     5:0,\n",
    "#     6:0,\n",
    "#     7:0,\n",
    "#     8:0,\n",
    "#     9:0,\n",
    "#     10:0,\n",
    "#     11:0,\n",
    "#     12:0\n",
    "# }\n",
    "classes_count_dict   = {\n",
    "    0:0,\n",
    "    1:0\n",
    "}\n",
    "\n",
    "for i,(input,labels,travel_time) in enumerate(train_dataloader):\n",
    "\n",
    "    _, labels_indices = torch.max(labels, 1)\n",
    "\n",
    "    for index in labels_indices:\n",
    "        # print(int(index.numpy()))\n",
    "        classes_count_dict[int(index.numpy())]+=1\n",
    "\n",
    "\n",
    "print(classes_count_dict)\n",
    "class_counts = []\n",
    "for key in classes_count_dict.keys():\n",
    "\n",
    "    class_counts.append(classes_count_dict[key])\n",
    "\n",
    "weights = 1.0 / torch.tensor(class_counts, dtype=torch.float,device=device)\n",
    "weights = weights / weights.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "num_epochs  = 40\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "model = CNNClassifier()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=weights)\n",
    "# loss_function = FocalLoss(alpha=weights, gamma=3)\n",
    "lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "correct_predictions_per_class = torch.zeros(n_classes, device=device)\n",
    "actual_per_class = torch.zeros(n_classes, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, input_size=train_dataloader[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Device used is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "training_loss_data = [] # Stores total training loss for each epoch\n",
    "testing_loss_data = [] # Stores total testing loss for each epoch\n",
    "epochs = []\n",
    "training_time = 0 # Stores the total training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training started for {num_epochs} epochs\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epochs .append(epoch) \n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "    total_predictions =0\n",
    "    \n",
    "    for i, (inputs, labels,travel_time) in enumerate(train_dataloader):\n",
    "        \n",
    "        inputs, labels,travel_time = inputs.to(device), labels.to(device) ,travel_time.to(device) # Move data to the device\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # print(inputs.shape)\n",
    "        # print(travel_time.shape)\n",
    "        outputs = model(inputs,travel_time)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_indices = torch.max(labels, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        predicted = predicted.to(labels_indices.device).long() \n",
    "        labels_indices = labels_indices.to(predicted.device).long()\n",
    "\n",
    "        correct_predictions += (predicted == labels_indices).sum().item()\n",
    "        total_predictions = total_predictions+32\n",
    "        \n",
    "        for class_index in range(n_classes):\n",
    "            correct_predictions_per_class[class_index] += ((predicted == labels_indices) & (labels_indices == class_index)).sum()\n",
    "            actual_per_class[class_index] += (labels_indices == class_index).sum()\n",
    "\n",
    "        if i % 152 == 0:  # Log every 10 batches\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1}/{len(train_dataloader)}, Partial Loss: {running_loss/(i+1):.4f}, Correct Predictions: {correct_predictions}/{total_predictions}\")\n",
    "            \n",
    "\n",
    "        global_step+=1\n",
    "        \n",
    "    # before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    # scheduler.step()\n",
    "    # after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    training_loss_data.append(epoch_loss)\n",
    "    epoch_accuracy = (correct_predictions / total_samples) * 100\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds.\")\n",
    "    # print(f\"Before Learning rate {before_lr}, After Learning rate {after_lr}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    writer.add_scalar(\"Loss/train\",epoch_loss, global_step) # Writing to tensorboard\n",
    "    # Testing phase ---------------------------------------------------------------------------\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        correct_test_predictions = 0\n",
    "        total_test_samples = 0\n",
    "        test_loss = 0\n",
    "        for test_inputs, test_labels,test_travel_time in test_dataloader:\n",
    "            test_inputs, test_labels,test_travel_time = test_inputs.to(device), test_labels.to(device), test_travel_time.to(device)      \n",
    "            outputs = model(test_inputs,test_travel_time)\n",
    "            loss = loss_function(outputs, test_labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test_samples += test_labels.size(0)\n",
    "            _, labels_indices = torch.max(test_labels, 1)\n",
    "\n",
    "            correct_test_predictions += (predicted == labels_indices).sum().item()\n",
    "    testing_loss_data.append(test_loss/len(test_dataloader))\n",
    "    test_accuracy = (correct_test_predictions / total_test_samples) * 100\n",
    "    print(f\"Epoch {epoch+1} Test Accuracy: {test_accuracy:.2f}% Test Loss {test_loss/len(test_dataloader)}\\n\")\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss vs epochs\n",
    "plt.plot(epochs,training_loss_data)\n",
    "plt.plot(epochs,testing_loss_data)\n",
    "plt.title(\"Training loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loss vs epochs\n",
    "plt.plot(epochs,testing_loss_data)\n",
    "plt.title(\"Testing loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running evaluation on test model again\n",
    "true_labels = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels,traveltime in test_dataloader:\n",
    "        test_inputs, test_labels,test_time = inputs.to(device), labels.to(device)  ,traveltime.to(device)\n",
    "        outputs = model(test_inputs,test_time) # Get model outputs for the current batch\n",
    "        _, predicted = torch.max(outputs, 1) # Get the index of the max log-probability\n",
    "        _, ground_truth = torch.max(labels, 1)\n",
    "        # Store predictions and true labels\n",
    "        true_labels.extend(ground_truth.cpu().numpy())\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "true_labels = np.array(true_labels)\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "# to get heat map\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", square=True,\n",
    "            xticklabels=range(np.unique(true_labels).size),\n",
    "            yticklabels=range(np.unique(true_labels).size))\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score and other metrics\n",
    "\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "\n",
    "torch.save(model, f\"saved_models/V{experiment_no}/model.pth\")\n",
    "with open(f\"saved_models/V{experiment_no}/README.md\",\"w\") as f:\n",
    "    f.write(f\"Model details:\\n\")\n",
    "    f.write(f\"Train accuracy : {epoch_accuracy}\\n\")\n",
    "    f.write(f\"Test accuracy : {test_accuracy}\\n\")\n",
    "    f.write(f\"Precision : {precision}\\n\")\n",
    "    f.write(f\"Recall : {recall}\\n\")\n",
    "    f.write(f\"F1 : {f1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model details:\")\n",
    "print(f\"Train accuracy : {epoch_accuracy}\")\n",
    "print(f\"Test accuracy : {test_accuracy}\")\n",
    "print(f\"Precision : {precision}\")\n",
    "print(f\"Recall : {recall}\")\n",
    "print(f\"F1 : {f1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running evaluation on test model again\n",
    "true_labels = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels,traveltime in test_dataloader:\n",
    "        test_inputs, test_labels,test_time = inputs.to(device), labels.to(device)  ,traveltime.to(device)\n",
    "        outputs = model(test_inputs,test_time) # Get model outputs for the current batch\n",
    "        _, predicted = torch.max(outputs, 1) # Get the index of the max log-probability\n",
    "        _, ground_truth = torch.max(labels, 1)\n",
    "        # Store predictions and true labels\n",
    "        print(predicted)\n",
    "        \n",
    "        \n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
